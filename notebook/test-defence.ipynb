{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12087525,"sourceType":"datasetVersion","datasetId":7609166}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/required8')\nimport matplotlib\nmatplotlib.use('Agg') \nfrom scipy.stats import entropy\nfrom numpy.linalg import norm\nfrom matplotlib.ticker import FuncFormatter\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pylab\nimport os\nimport matplotlib.pyplot as plt\n\nclass AEDetector:\n    def __init__(self, model_class, model_path, p=1, device='cpu',model_kwargs=None):\n        \"\"\"\n        Error-based detector.\n\n        model_class: The class of the AE model (e.g., DenoisingAutoEncoder).\n        model_path: Path to the model's saved state_dict (.pth).\n        p: Power for error norm (e.g., 1 = L1, 2 = L2).\n        device: torch.device object or string (\"cuda\" / \"cpu\").\n        \"\"\"\n        self.device = device \n        self.model = model_class(**(model_kwargs or {})).to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        self.p = p\n        \n    def mark(self, X):\n        \"\"\"\n        X: A PyTorch tensor of shape [N, C, H, W] on CPU or GPU.\n        Returns: 1D numpy array of anomaly scores (reconstruction errors).\n        \"\"\"\n        self.model.eval()\n\n        with torch.no_grad():\n            if X.ndim == 4 and X.shape[1:] == torch.Size([28, 28, 1]):\n                \n                X = X.permute(0, 3, 1, 2)\n\n            X = X.to(self.device)\n            recon = self.model(X)\n            diff = torch.abs(X - recon)  # Absolute error\n            score = torch.mean(torch.pow(diff, self.p), dim=[1, 2, 3])  # Per-sample score\n            return score.cpu().numpy()  # Convert to NumPy for compatibility\n\n    def print(self):\n        return \"AEDetector:\" + os.path.basename(self.path)\n\n\nclass IdReformer:\n    def __init__(self, device=None):\n        \"\"\"\n        Improved identity reformer with device handling\n        \"\"\"\n        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n    def heal(self, X):\n        \"\"\"Maintain device consistency\"\"\"\n        return X.to(self.device)\n\n    def print(self):\n        return f\"IdReformer(device={self.device})\"\n\n\nclass SimpleReformer:\n    def __init__(self, model_class, model_path, device='cpu', model_kwargs=None):\n        \"\"\"\n        Autoencoder-based reformer. Applies reconstruction (healing).\n\n        model_class: Class definition of the autoencoder.\n        path: Path to the saved model (.pth).\n        device: torch.device or str (\"cuda\" or \"cpu\").\n        \"\"\"\n        self.device = device \n        self.model = model_class(**(model_kwargs or {})).to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n        self.image_shape = self.model.image_shape\n        self.path = model_path\n\n    def heal(self, X):\n        \"\"\"\n        X: Tensor [N, C, H, W] (should be on same device as model)\n        Returns: Reconstructed input clipped between 0 and 1 (as Torch Tensor)\n        \"\"\"\n        self.model.eval()\n        with torch.no_grad():\n            X = X.to(self.device)\n            if X.ndim == 3:  # Single image\n                if X.shape[-1] == 1:  # HWC format\n                    X = X.permute(2, 0, 1).unsqueeze(0)  # Convert to NCHW\n                else:  # CHW format\n                    X = X.unsqueeze(0)  # Add batch dimension\n            elif X.ndim == 4:  # Batch of images\n                if X.shape[-1] == 1:  # NHWC format\n                    X = X.permute(0, 3, 1, 2)  # Convert to NCHW\n        \n       \n            recon = self.model(X)\n            recon = torch.clamp(recon, 0.0, 1.0)\n            return recon.cpu()\n\n    def print(self):\n        return \"SimpleReformer:\" + os.path.basename(self.path)\n\n\ndef JSD(P, Q):\n    \"\"\"\n    Jensen-Shannon Divergence between two 1D arrays (P, Q).\n    P, Q: Numpy arrays representing discrete distributions.\n    \"\"\"\n    _P = P / norm(P, ord=1)\n    _Q = Q / norm(Q, ord=1)\n    _M = 0.5 * (_P + _Q)\n    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n\n\nclass DBDetector:\n    def __init__(self, reconstructor, prober, classifier, option=\"jsd\", T=1):\n        \"\"\"\n        Divergence-Based Detector using PyTorch.\n\n        reconstructor: One reformer (e.g., SimpleReformer instance).\n        prober: Another reformer (same type).\n        classifier: Classifier object with classify() method.\n        option: Distance measure to use, default is 'jsd'.\n        T: Temperature for softmax (scaling logits).\n        \"\"\"\n        self.prober = prober\n        self.reconstructor = reconstructor\n        self.classifier = classifier\n        self.option = option\n        self.T = T\n\n    def mark(self, X):\n        if self.option == \"jsd\":\n            return self.mark_jsd(X)\n        else:\n            raise NotImplementedError(f\"Only 'jsd' is implemented, got {self.option}\")\n\n    def mark_jsd(self, X):\n        \"\"\"\n        Returns JSD divergence between classifier outputs on\n        probed and reconstructed samples.\n        \n        X: Input tensor [N, C, H, W] (torch.Tensor)\n        \"\"\"\n        Xp = self.prober.heal(X)\n        Xr = self.reconstructor.heal(X)\n\n        Pp = self.classifier.classify(Xp, option=\"prob\", T=self.T)  # numpy array [N, num_classes]\n        Pr = self.classifier.classify(Xr, option=\"prob\", T=self.T)\n\n        marks = [JSD(Pp[i], Pr[i]) for i in range(len(Pr))]\n        return np.array(marks)\n\n    def print(self):\n        return \"Divergence-Based Detector\"\n\n\n\nclass Classifier:\n    def __init__(self, model_class, classifier_path, device='cpu', model_kwargs=None):\n        \"\"\"\n        PyTorch classifier wrapper. Assumes the model outputs raw logits.\n\n        model_class: A callable (e.g., a class or lambda) that returns the classifier architecture.\n        classifier_path: Path to saved model weights (.pth).\n        device: torch.device or string (\"cuda\"/\"cpu\"). Auto-detected if not provided.\n        model_kwargs: dict of kwargs for model_class constructor\n        \"\"\"\n        self.path = classifier_path\n        self.device = device \n\n        self.model = model_class(**(model_kwargs or {})).to(self.device)\n        self.model.load_state_dict(torch.load(classifier_path, map_location=self.device))\n        self.model.eval()\n\n    def classify(self, X, option=\"logit\", T=1):\n        \"\"\"\n        Classify input.\n\n        X: Torch tensor [N, C, H, W]\n        option: \"logit\" to return raw logits, \"prob\" to return softmax probs.\n        T: Temperature (used only with option=\"prob\")\n        Returns: NumPy array\n        \"\"\"\n        if X.ndim == 4 and X.shape[1:] == torch.Size([28, 28, 1]):\n            X = X.permute(0, 3, 1, 2)\n        self.model.eval()\n        with torch.no_grad():\n            if isinstance(X, np.ndarray):\n                X = torch.from_numpy(X).float()\n\n            X = X.to(self.device)\n            logits = self.model(X)\n\n            if option == \"logit\":\n                return logits.cpu().numpy()\n\n            elif option == \"prob\":\n                probs = F.softmax(logits / T, dim=1)\n                return probs.cpu().numpy()\n\n            else:\n                raise ValueError(f\"Invalid option: {option}. Use 'logit' or 'prob'.\")\n\n    def print(self):\n        return \"Classifier:\" + os.path.basename(self.path)\n\n\nclass Operator:\n    def __init__(self, data, classifier, det_dict, reformer):\n        \"\"\"\n        Operator that wraps the data, classifier, detector(s), and reformer logic.\n\n        data: MNIST object with .train_loader, .validation_loader, .test_loader\n        classifier: Classifier object (must have .classify(X) method)\n        det_dict: Dictionary of detectors, each with .mark(X)\n        reformer: Reformer object (must have .heal(X))\n        \"\"\"\n        self.data = data\n        self.classifier = classifier\n        self.det_dict = det_dict\n        self.reformer = reformer\n\n        test_imgs = torch.stack([img for img, _ in data.test_data])\n        test_labels = torch.tensor([label for _, label in data.test_data])\n        self.normal = self.operate(AttackData(test_imgs, test_labels, \"Normal\"))\n\n    def get_thrs(self, drop_rate):\n        \"\"\"\n        Calculates thresholds for filtering from validation set.\n        drop_rate: Dict mapping detector names to float drop rates.\n        \"\"\"\n        thrs = {}\n        val_imgs = torch.stack([img for img, _ in self.data.validation_data])\n        for name, detector in self.det_dict.items():\n            num = int(len(val_imgs) * drop_rate[name])\n            marks = detector.mark(val_imgs)\n            sorted_marks = np.sort(marks)\n            thrs[name] = sorted_marks[-num]\n        return thrs\n\n    def operate(self, untrusted_obj):\n        \"\"\"\n        Classifies original and reformed inputs using the classifier.\n        Returns: Array of (original_correct, reformed_correct) pairs.\n        \"\"\"\n        device = next(self.classifier.model.parameters()).device\n        X = untrusted_obj.data.to(device)\n        Y_true = untrusted_obj.labels.to(device)\n\n        with torch.no_grad():\n            X_prime = self.reformer.heal(X)\n            Y_pred = torch.tensor(np.argmax(self.classifier.classify(X), axis=1))\n            Y_judgement = (Y_pred == Y_true[:len(X_prime)])\n\n            Yp_pred = torch.tensor(np.argmax(self.classifier.classify(X_prime), axis=1))\n            Yp_judgement = (Yp_pred == Y_true[:len(X_prime)])\n\n        return np.array(list(zip(Y_judgement.cpu().numpy(), Yp_judgement.cpu().numpy())))\n\n    def filter(self, X, thrs):\n        \"\"\"\n        Filters inputs using detector thresholds.\n        Returns indices that passed all filters.\n        \"\"\"\n        all_pass = np.arange(X.shape[0])\n        collector = {}\n\n        for name, detector in self.det_dict.items():\n            marks = detector.mark(X)\n            idx_pass = np.argwhere(marks < thrs[name]).flatten()\n            collector[name] = len(idx_pass)\n            all_pass = np.intersect1d(all_pass, idx_pass)\n\n        return all_pass, collector\n\n    def print(self):\n        components = [self.reformer, self.classifier]\n        return \" \".join(obj.print() for obj in components)\n\n\n\nclass Evaluator:\n    def __init__(self, operator, untrusted_data, graph_dir=\"/kaggle/working/graphs/\"):\n        \"\"\"\n        Evaluator for analyzing the defense strategy.\n\n        operator: Operator object.\n        untrusted_data: Adversarial or noisy test dataset wrapped in AttackData.\n        graph_dir: Path to save graphs.\n        \"\"\"\n        self.operator = operator\n        self.untrusted_data = self.normalize_attack_data(untrusted_data)\n        self.graph_dir = graph_dir\n        self.data_package = operator.operate(untrusted_data)\n\n    def normalize_attack_data(self, data):\n        \"\"\"Normalize AttackData images\"\"\"\n        device = next(self.operator.classifier.model.parameters()).device\n        normalized_images = normalize_mnist(data.data.to(device))\n        return AttackData(normalized_images, data.labels.to(device), data.name)\n\n\n    def bind_operator(self, operator):\n        \"\"\"\n        Replace current operator and re-evaluate.\n        \"\"\"\n        self.operator = operator\n        self.data_package = operator.operate(self.untrusted_data)\n\n    def load_data(self, data):\n        \"\"\"\n        Replace current untrusted data and re-evaluate.\n        \"\"\"\n        self.untrusted_data = data\n        self.data_package = self.operator.operate(self.untrusted_data)\n\n    def get_normal_acc(self, normal_all_pass):\n        \"\"\"\n        Measure classification accuracy on clean data after filtering.\n\n        Returns:\n        - both_acc: Accuracy when both detector and reformer pass.\n        - det_only_acc: Accuracy with just detector.\n        - ref_only_acc: Accuracy with just reformer.\n        - none_acc: Accuracy without any defense.\n        \"\"\"\n        normal_tups = self.operator.normal\n        num_normal = len(normal_tups)\n        filtered_normal_tups = normal_tups[normal_all_pass]\n\n        both_acc = sum(1 for _, XpC in filtered_normal_tups if XpC) / num_normal\n        det_only_acc = sum(1 for XC, _ in filtered_normal_tups if XC) / num_normal\n        ref_only_acc = sum(1 for _, XpC in normal_tups if XpC) / num_normal\n        none_acc = sum(1 for XC, _ in normal_tups if XC) / num_normal\n\n        return both_acc, det_only_acc, ref_only_acc, none_acc\n\n    def get_attack_acc(self, attack_pass):\n        \"\"\"\n        Measure classification accuracy on adversarial data.\n\n        Returns same metrics as get_normal_acc.\n        \"\"\"\n        attack_tups = self.data_package\n        num_untrusted = len(attack_tups)\n        filtered_attack_tups = attack_tups[attack_pass]\n\n        both_acc = 1 - sum(1 for _, XpC in filtered_attack_tups if not XpC) / num_untrusted\n        det_only_acc = 1 - sum(1 for XC, _ in filtered_attack_tups if not XC) / num_untrusted\n        ref_only_acc = sum(1 for _, XpC in attack_tups if XpC) / num_untrusted\n        none_acc = sum(1 for XC, _ in attack_tups if XC) / num_untrusted\n\n        return both_acc, det_only_acc, ref_only_acc, none_acc\n\n\n\n   \n\n    def plot_epsilon_sweep(self, graph_name, drop_rate, epsilons, \n                          attack_type=\"fgsm\", data_dir='/kaggle/working/attack_data/'):\n        \"\"\"Plots defense performance across different attack strengths (epsilons)\"\"\"\n        pylab.rcParams['figure.figsize'] = 10, 6\n        fig = plt.figure()\n        device = next(self.operator.classifier.model.parameters()).device\n        \n        # Initialize storage for metrics\n        metrics = {\n            'both': [],\n            'det_only': [],\n            'ref_only': [],\n            'none': []\n        }\n        \n        # Calculate dynamic thresholds\n        thrs = {\n            \"I\": 1.5,\n            \"II\":1.5\n        }\n        \n        for eps in epsilons:\n            # Load attack data\n            attack_name = f\"{attack_type}_{eps}\"\n            attack_data = load_obj(f\"{attack_name}_attack\", data_dir).to(device)\n            attack_labels = load_obj(f\"{attack_name}_labels\", data_dir).to(device)\n            \n            # Normalize and process\n            attack_data = normalize_mnist(attack_data)\n            attack_dataset = AttackData(attack_data, attack_labels, name=attack_name)\n            self.load_data(attack_dataset)\n            \n            # Get accuracy metrics\n            attack_pass, _ = self.operator.filter(attack_dataset.data, thrs)\n            total_images = len(attack_dataset.data)\n            passed_images = len(attack_pass)  # If attack_pass is indices\n        # OR if attack_pass is boolean mask: passed_images = sum(attack_pass)\n        \n            print(f\"ε={eps}: {passed_images}/{total_images} ({passed_images/total_images:.1%}) images passed detector\")\n            accs = self.get_attack_acc(attack_pass)\n            \n            # Store results\n            metrics['both'].append(accs[0])\n            metrics['det_only'].append(accs[1])\n            metrics['ref_only'].append(accs[2])\n            metrics['none'].append(accs[3])\n        \n        # Plot results\n        plt.plot(epsilons, metrics['both'], 'b-o', label=\"Both Defense\")\n        plt.plot(epsilons, metrics['det_only'], 'g-s', label=\"Detector Only\")\n        plt.plot(epsilons, metrics['ref_only'], 'r-^', label=\"Reformer Only\")\n        plt.plot(epsilons, metrics['none'], 'k-x', label=\"No Defense\")\n        \n        plt.xlabel(\"Attack Strength (ε)\")\n        plt.ylabel(\"Classification Accuracy\")\n        plt.title(\"Defense Performance vs FGSM Attack Strength\")\n        plt.legend()\n        plt.grid(True)\n        \n        # Save plot\n        graph_path = os.path.join(self.graph_dir, graph_name + \".png\")\n        os.makedirs(self.graph_dir, exist_ok=True)\n        plt.savefig(graph_path)\n        plt.close(fig)\n    \n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T17:37:01.604535Z","iopub.execute_input":"2025-06-07T17:37:01.605105Z","iopub.status.idle":"2025-06-07T17:37:01.641150Z","shell.execute_reply.started":"2025-06-07T17:37:01.605085Z","shell.execute_reply":"2025-06-07T17:37:01.640569Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"import pickle\nimport os\nclass AttackData:\n    def __init__(self, data, labels, name=\"\",directory=\"/kaggle/input/required8\"):\n        \"\"\"\n        Wrapper for input data (normal or adversarial).\n        \n        examples: Tensor or path to saved object.\n        labels: Tensor or numpy array of ground truth labels.\n        name: Identifier string.\n        \"\"\"\n        if not isinstance(data, torch.Tensor):\n            self.data = torch.tensor(data)\n        else:\n            self.data = data\n            \n        if not isinstance(labels, torch.Tensor):\n            self.labels = torch.tensor(labels)\n        else:\n            self.labels = labels\n            \n        self.name = name\n        self.length = len(self.data)\n\n    def print(self):\n        return \"Attack:\" + self.name\n\n\ndef normalize_mnist(images):\n    mean = torch.tensor([0.1307], device=images.device)\n    std = torch.tensor([0.3081], device=images.device)\n    return (images - mean.view(1, -1, 1, 1)) / std.view(1, -1, 1, 1)\n\n\n\ndef prepare_data(dataset, idx):\n    \"\"\"\n    Extracts image and label data at specific indices from the test set.\n\n    dataset: Instance of MNIST() class.\n    idx: List or tensor of indices into the test set.\n    Returns:\n        X: torch.Tensor of images [N, 1, 28, 28]\n        targets: torch.Tensor of integer labels [N]\n        Y: Same as targets (for compatibility with original code using argmax)\n    \"\"\"\n    # dataset.test_data is a torchvision.datasets.MNIST object\n    images = []\n    labels = []\n    for i in idx:\n        img, label = dataset.test_data[i]\n        images.append(img)\n        labels.append(label)\n\n    X = torch.stack(images)  # shape: [N, 1, 28, 28]\n    targets = torch.tensor(labels, dtype=torch.long)  # shape: [N]\n    Y = targets.clone()  # No one-hot, so argmax would just return same\n\n    return X, targets, Y\n\n\ndef save_obj(obj, name, directory='./attack_data/'):\n    os.makedirs(directory, exist_ok=True)\n    with open(os.path.join(directory, name + '.pkl'), 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\n\ndef load_obj(name, directory='./attack_data/'):\n    if name.endswith(\".pkl\"):\n        name = name[:-4]\n    with open(os.path.join(directory, name + '.pkl'), 'rb') as f:\n        return pickle.load(f)\n\n    if isinstance(data, (np.ndarray, memoryview)):\n        data = torch.tensor(np.array(data))\n    elif isinstance(data, list):\n        data = torch.stack([torch.tensor(x) for x in data])\n    \n    return data\n\n\ndef generate_attack_data(model, attack_type, epsilon, steps=10, num_samples=1000):\n    \"\"\"\n    Generate adversarial examples using FGSM/PGD\n    Returns: AttackData object containing adversarial examples\n    \"\"\"\n    # Get clean samples\n    idx = torch.randperm(len(dataset.test_data))[:num_samples]\n    X_clean, targets, Y = prepare_data(dataset, idx)\n    \n    # Initialize attack\n    if attack_type.lower() == \"fgsm\":\n        attacker = FGSMAttack(model, epsilon=epsilon)\n    elif attack_type.lower() == \"pgd\":\n        attacker = PGDAttack(model, eps=epsilon, alpha=epsilon/4, steps=steps)\n    else:\n        raise ValueError(f\"Unsupported attack type: {attack_type}\")\n\n    # Generate adversarial examples\n    X_adv = attacker.generate(X_clean, targets)\n    \n    return AttackData(X_adv, targets, name=f\"{attack_type.upper()}_eps_{epsilon}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T17:37:01.642407Z","iopub.execute_input":"2025-06-07T17:37:01.642636Z","iopub.status.idle":"2025-06-07T17:37:01.663392Z","shell.execute_reply.started":"2025-06-07T17:37:01.642617Z","shell.execute_reply":"2025-06-07T17:37:01.662634Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"class FGSMAttack:\n    def __init__(self, model, epsilon=0.2):\n        self.model = model\n        self.epsilon = epsilon\n        self.device = next(model.parameters()).device\n        \n    def generate(self, images, labels):\n        images = images.clone().detach().to(self.device)\n        labels = labels.clone().detach().to(self.device)\n        images.requires_grad = True\n        \n        outputs = self.model(images)\n        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n        \n        self.model.zero_grad()\n        loss.backward()\n        \n        perturbation = self.epsilon * images.grad.data.sign()\n        perturbed_images = torch.clamp(images + perturbation, 0, 1).detach()\n        \n        return perturbed_images\n\nclass PGDAttack:\n    def __init__(self, model, eps=0.1, alpha=0.01, steps=10):\n        self.model = model\n        self.eps = eps\n        self.alpha = alpha\n        self.steps = steps\n        self.device = next(model.parameters()).device\n        \n    def generate(self, images, labels):\n        images = images.clone().detach().to(self.device)\n        labels = labels.clone().detach().to(self.device)\n        \n        delta = torch.empty_like(images).uniform_(-self.eps, self.eps)\n        adv_images = torch.clamp(images + delta, 0, 1).detach()\n        \n        for _ in range(self.steps):\n            adv_images.requires_grad = True\n            outputs = self.model(adv_images)\n            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n            \n            self.model.zero_grad()\n            loss.backward()\n            \n            grad = adv_images.grad.detach()\n            delta = adv_images - images + self.alpha * grad.sign()\n            delta = torch.clamp(delta, -self.eps, self.eps)\n            adv_images = torch.clamp(images + delta, 0, 1).detach()\n            \n        return adv_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T17:37:01.664137Z","iopub.execute_input":"2025-06-07T17:37:01.664385Z","iopub.status.idle":"2025-06-07T17:37:01.683096Z","shell.execute_reply.started":"2025-06-07T17:37:01.664363Z","shell.execute_reply":"2025-06-07T17:37:01.682387Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/required8')\nimport torch\nfrom dataloader import MNIST\nfrom utils import prepare_data, load_obj\nfrom DAE_model import DenoisingAutoEncoder\nfrom model import CNNClassifier\n\n\n\n# ---- Load models ----\ndetector_I = AEDetector(DenoisingAutoEncoder, \"/kaggle/input/required8/MNIST_I.pth\", p=2, model_kwargs={'image_shape': (1, 28, 28), 'structure': [3,\"average\",3], 'v_noise': 0.1, 'activation': 'relu', 'model_dir': './defensive_models/', 'reg_strength': 0.0})\ndetector_II = AEDetector(DenoisingAutoEncoder, \"/kaggle/input/required8/MNIST_II.pth\", p=1, model_kwargs={'image_shape': (1, 28, 28), 'structure': [3], 'v_noise': 0.1, 'activation': 'relu', 'model_dir': './defensive_models/', 'reg_strength': 0.0})\nreformer = SimpleReformer(DenoisingAutoEncoder, \"/kaggle/input/required8/MNIST_I.pth\", model_kwargs={'image_shape': (1, 28, 28), 'structure': [3,\"average\",3], 'v_noise': 0.1, 'activation': 'relu', 'model_dir' : './defensive_models/', 'reg_strength': 0.0})\n\"\"\"idreformer = IdReformer(device=next(classifier.model.parameters()).device)\"\"\"\nclassifier = Classifier(CNNClassifier, \"/kaggle/input/required8/example_classifier.pth\",model_kwargs={'params' : [32,32,64,64,200,200]})\n\n# ---- Compose detector dictionary ----\ndetector_dict = {\n    \"I\": detector_I,\n    \"II\": detector_II\n}\n\n# ---- Load MNIST data ----\ndataset = MNIST()\noperator = Operator(dataset, classifier, detector_dict, reformer)\n\nSAVE_DIR = '/kaggle/working/attack_data/'\nos.makedirs(SAVE_DIR, exist_ok=True)\n\n# Generate 1000 random indices\nidx = torch.randperm(len(dataset.test_data))[:1000]\nX_clean, targets, Y = prepare_data(dataset, idx)\nX_clean = normalize_mnist(X_clean)\n\nepsilons = [0.01, 0.05, 0.1, 0.2, 0.5]\n# Generate attacks\nfor eps in epsilons:\n    fgsm_attack = generate_attack_data(classifier.model, \"fgsm\", eps, num_samples=1000)\n    save_obj(fgsm_attack.data.cpu(), f\"fgsm_{eps}_attack\", directory=SAVE_DIR)\n    save_obj(fgsm_attack.labels.cpu(), f\"fgsm_{eps}_labels\", directory=SAVE_DIR)\n\n\n\n# Evaluation\nLOAD_DIR = '/kaggle/working/attack_data/'\ndevice = next(classifier.model.parameters()).device\ndef load_and_normalize_attack(attack_name):\n    images = load_obj(f\"{attack_name}_attack\", LOAD_DIR).to(device)\n    labels = load_obj(f\"{attack_name}_labels\", LOAD_DIR).to(device)\n    return AttackData(normalize_mnist(images), labels, name=attack_name)\n\n\n\n\n\ninitial_attack = load_and_normalize_attack(\"fgsm_0.1\")\nevaluator = Evaluator(operator, initial_attack)\n\n# Call the epsilon sweep plot\nevaluator.plot_epsilon_sweep(\n    epsilons=[0.01, 0.05, 0.1, 0.2, 0.5],\n    drop_rate={\"I\": 0.1, \"II\": 0.1},\n    graph_name=\"fgsm_epsilon_comparison\"\n)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-07T17:37:01.683743Z","iopub.execute_input":"2025-06-07T17:37:01.683991Z","iopub.status.idle":"2025-06-07T17:37:29.131934Z","shell.execute_reply.started":"2025-06-07T17:37:01.683966Z","shell.execute_reply":"2025-06-07T17:37:29.131309Z"}},"outputs":[{"name":"stdout","text":"ε=0.01: 956/1000 (95.6%) images passed detector\nε=0.05: 972/1000 (97.2%) images passed detector\nε=0.1: 979/1000 (97.9%) images passed detector\nε=0.2: 994/1000 (99.4%) images passed detector\nε=0.5: 1000/1000 (100.0%) images passed detector\n","output_type":"stream"}],"execution_count":99},{"cell_type":"code","source":"%matplotlib inline\nclean_img = dataset.test_data[10][0].unsqueeze(0).to(reformer.device)  # Shape: [1, 1, 28, 28]\nreformed = reformer.heal(clean_img)\nplt.subplot(1,2,1); plt.imshow(clean_img.cpu().squeeze(), cmap='gray')\nplt.title(\"Clean Input\")\nplt.subplot(1,2,2); plt.imshow(reformed.cpu().squeeze(), cmap='gray')\nplt.title(\"Reformed Output\")\nplt.show()\n\ndevice = next(classifier.model.parameters()).device\nsample_idx = 10\n# Load and convert adversarial data\nadv_imgs = load_obj(\"fgsm_attack\", directory='/kaggle/working/attack_data/')\nadv_labels = load_obj(\"fgsm_labels\", directory='/kaggle/working/attack_data/')\n\n# Convert to tensors and move to device\nadv_imgs = adv_imgs.to(device) if isinstance(adv_imgs, torch.Tensor) else torch.tensor(adv_imgs).to(device)\nadv_labels = adv_labels.to(device) if isinstance(adv_labels, torch.Tensor) else torch.tensor(adv_labels).to(device)\nadv_imgs = normalize_mnist(adv_imgs)\nadv_sample = adv_imgs[sample_idx].unsqueeze(0)  # [1,1,28,28]\nreformed_sample = reformer.heal(adv_sample)\n\n\nadv_display = adv_sample.cpu().squeeze()\nreformed_display = reformed_sample.cpu().squeeze()\ndifference = (adv_display - reformed_display).abs()\n\n# Plot\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 3, 1)\nplt.imshow(adv_display, cmap='gray', vmin=0, vmax=1)\nplt.title(f\"Adversarial Input (Sample {sample_idx})\")\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.imshow(reformed_display, cmap='gray', vmin=0, vmax=1)\nplt.title(\"Reformed Output\")\nplt.axis('off')\n\n\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T17:37:29.133574Z","iopub.execute_input":"2025-06-07T17:37:29.133818Z","iopub.status.idle":"2025-06-07T17:37:29.529154Z","shell.execute_reply.started":"2025-06-07T17:37:29.133778Z","shell.execute_reply":"2025-06-07T17:37:29.528535Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAwsAAAGXCAYAAAD8ncyrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAs4UlEQVR4nO3deZQU9b028G/PwjDDjgyKqIigJoomXryKBgSXK6hIMCKLxCUu0bjnvGpMchLFqFlQY25coolBo6ghqInxukSjWYwSFXPdEhQNuAdQUFkcYWbq/cMzfR3mB051YAbx8znHc6Sop+rXRU9VP1XVNYUsy7IAAABYTVl7DwAAANgwKQsAAECSsgAAACQpCwAAQJKyAAAAJCkLAABAkrIAAAAkKQsAAECSsgAAACQpC+vJddddF4VCIebPn9/eQ1mvCoVCnHfeeblzf/jDH6JQKMQf/vCHdT6mDdGMGTOiZ8+esWzZsvYeyjpx3nnnRaFQaO9hrNU999wTnTt3jkWLFrX3UOATb+7cubH//vtHt27dolAoxK9//ev2HtI68Uk7lvHJpCx8hCuvvDIKhULsvvvu7T2UT6Sm0vX444+391AiImLFihVx3nnn5TowNDQ0xLnnnhunnnpqdO7cuTh95cqV8aMf/Sh22WWX6Nq1a3Tv3j123HHH+PKXvxxz5sxZD6PfcC1btizOPffcGDVqVPTs2TMKhUJcd911a5z/H//4R4waNSo6d+4cPXv2jCOOOKJFKRg1alQMHDgwvvvd767n0cPGo2mf2/RfRUVF9O3bN44++uh47bXXSl7uUUcdFU8//XRceOGFccMNN8Suu+66Dkf98fHss8/GF7/4xejbt29UVVXF5ptvHpMnT45nn33231ruRRdd1GYF7OGHH47zzjsv3n777TZZH+2vor0HsKGbPn16bL311vHoo4/GCy+8EAMHDmzvIW1Q3nvvvaio+OS8jVasWBFTpkyJiIgRI0a0KvPb3/42nnvuufjyl7/cbPqhhx4ad999d0yaNCmOP/74WLVqVcyZMyfuvPPO2HPPPeNTn/rUuh7+BuvNN9+M888/P7baaqv4zGc+s9Yy9uqrr8Zee+0V3bp1i4suuiiWLVsWF198cTz99NPx6KOPRocOHYrznnDCCXHmmWfGlClTokuXLm3wSmDjcP7550f//v2jrq4uZs2aFdddd1089NBD8cwzz0THjh1zLeu9996LRx55JL75zW/GKaecsp5GvOG77bbbYtKkSdGzZ8849thjo3///jF//vy49tprY+bMmXHLLbfEIYccUtKyL7roohg3blyMHTt23Q464eGHH44pU6bE0UcfHd27d1/v66P9fXI+5ZVg3rx58fDDD8dtt90WJ5xwQkyfPj3OPffc9h7WWq1YsSJqamrW6zoaGxtj5cqV0bFjx9wHjU+iadOmxec+97no27dvcdpjjz0Wd955Z1x44YXxjW98o9n8l19++SfujE2fPn3ijTfeiM022ywef/zx+M///M81znvRRRfF8uXLY/bs2bHVVltFRMRuu+0W//Vf/xXXXXdds1J26KGHxqmnnhq/+tWv4phjjlnvrwM2FgcccEDx7P9xxx0XvXr1iu9///txxx13xPjx43Mtq+mq37r8YLl8+fLo1KnTOlve+vbiiy/GEUccEdtss0386U9/itra2uLfnX766TFs2LA44ogj4qmnnoptttmmHUcKLbkNaS2mT58ePXr0iIMOOijGjRsX06dPT8737LPPxj777BPV1dWxxRZbxAUXXBCNjY3N5hk9evQadwB77LFHi0uyN954YwwePDiqq6ujZ8+eMXHixHjllVeazTNixIgYNGhQzJ49O/baa6+oqakpfvB8/PHHY+TIkdGrV6+orq6O/v37t/iwdPHFF8eee+4Zm2yySVRXV8fgwYNj5syZLcZXKBTilFNOienTp8eOO+4YVVVVcc899xT/7sPfWXjppZfipJNOiu233z6qq6tjk002icMOO2ydfnfj6KOPjs6dO8drr70WY8eOjc6dO0dtbW2ceeaZ0dDQUJxv/vz5USgU4uKLL44f/vCH0a9fv6iuro7hw4fHM88802yZI0aMSF4pOProo2PrrbcuLq9pBz9lypTiZfq1fWejrq4u7rnnnthvv/2aTX/xxRcjIuJzn/tci0x5eXlssskmxT+3dps23T7w0EMPxWmnnRa1tbXRvXv3OOGEE2LlypXx9ttvx5FHHhk9evSIHj16xNlnnx1ZlpW0vdakNe/blKqqqthss81atY5bb701Ro8eXSwKERH77bdfbLfddjFjxoxm8/bu3Tt23nnn+M1vftOqZQNpw4YNi4j/23c1mTNnTowbNy569uwZHTt2jF133TXuuOOO4t+fd9550a9fv4iIOOuss6JQKBT3qRERf/vb3+KAAw6Irl27RufOnWPfffeNWbNmNVtH077tj3/8Y5x00knRu3fv2GKLLSLi/46DTz31VAwfPjxqampi4MCBxWPZH//4x9h9992juro6tt9++7j//vtbvLbXXnstjjnmmNh0002jqqoqdtxxx/j5z3/eYr5XX301xo4dG506dYrevXvHV7/61Xj//fdbtf2mTp0aK1asiGuuuaZZUYiI6NWrV1x99dWxfPny+MEPflCc/uHjz4et/p2xQqEQy5cvj+uvv754XDr66KObzTtnzpwYP358dO3aNTbZZJM4/fTTo66urriMpv1/6vbPDx/nzjvvvDjrrLMiIqJ///7F9W3s38/8pHNlYS2mT58eX/jCF6JDhw4xadKkuOqqq+Kxxx5rdtbzX//6V+y9995RX18f55xzTnTq1CmuueaaqK6ubrasCRMmxJFHHtki/9JLL8WsWbNi6tSpxWkXXnhhfOtb34rx48fHcccdF4sWLYof//jHsddee8Xf/va3Zmdn3nrrrTjggANi4sSJ8cUvfjE23XTTWLhwYey///5RW1sb55xzTnTv3j3mz58ft912W7Mx/ehHP4oxY8bE5MmTY+XKlXHLLbfEYYcdFnfeeWccdNBBzeZ94IEHYsaMGXHKKadEr169kjuwiA/OmD/88MMxceLE2GKLLWL+/Plx1VVXxYgRI+Lvf//7Orvq0dDQECNHjozdd989Lr744rj//vvjkksuiQEDBsRXvvKVZvP+4he/iKVLl8bJJ58cdXV18aMf/Sj22WefePrpp2PTTTdt9Tpra2vjqquuiq985StxyCGHxBe+8IWIiNh5553XmJk9e3asXLky/uM//qPZ9KaD5/Tp0+Nzn/vcWm/lyrtNTz311Nhss81iypQpMWvWrLjmmmuie/fu8fDDD8dWW20VF110Udx1110xderUGDRoUBx55JHrZHvled+W6rXXXouFCxcm73febbfd4q677moxffDgwRvNlymhvTR9GOzRo0dx2rPPPlu8atp0/JsxY0aMHTs2br311uJ+snv37vHVr341Jk2aFAceeGDxu1vPPvtsDBs2LLp27Rpnn312VFZWxtVXXx0jRowofsj/sJNOOilqa2vj29/+dixfvrw4fcmSJTF69OiYOHFiHHbYYXHVVVfFxIkTY/r06XHGGWfEiSeeGIcffnhMnTo1xo0bF6+88krxtsQFCxbEkCFDiifFamtr4+67745jjz023n333TjjjDMi4oNbqfbdd994+eWX47TTTovNN988brjhhnjggQdatf1++9vfxtZbb10sXavba6+9Yuutt47/+Z//adXyPuyGG26I4447LnbbbbfildUBAwY0m2f8+PGx9dZbx3e/+92YNWtW/Pd//3csWbIkfvGLX+Ra1xe+8IV4/vnn4+abb44f/vCH0atXr4iIFgWIjUxG0uOPP55FRHbfffdlWZZljY2N2RZbbJGdfvrpzeY744wzsojI/vrXvxanLVy4MOvWrVsWEdm8efOyLMuyd955J6uqqsr+3//7f83yP/jBD7JCoZC99NJLWZZl2fz587Py8vLswgsvbDbf008/nVVUVDSbPnz48Cwisp/85CfN5r399tuziMgee+yxtb7GFStWNPvzypUrs0GDBmX77LNPs+kRkZWVlWXPPvtsi2VERHbuueeucZlZlmWPPPJIFhHZL37xi+K0Bx98MIuI7MEHH1zrGKdNm9bitRx11FFZRGTnn39+s3l32WWXbPDgwcU/z5s3L4uIrLq6Onv11VeL0//6179mEZF99atfLU4bPnx4Nnz48BbrP+qoo7J+/foV/7xo0aIWr3ltfvazn2URkT399NPNpjc2Nhb//TbddNNs0qRJ2RVXXFF8H3xYa7dp07YaOXJk1tjYWJy+xx57ZIVCITvxxBOL0+rr67Mtttii2WvOs73OPffc7MO7jzzv24/y2GOPZRGRTZs2bY1/9+HX3eSss87KIiKrq6trNv2iiy7KIiJbsGBBq8cAn1RN+5H7778/W7RoUfbKK69kM2fOzGpra7OqqqrslVdeKc677777ZjvttFOzn7nGxsZszz33zLbddtvitKZ9y9SpU5uta+zYsVmHDh2yF198sTjt9ddfz7p06ZLttddeLcY0dOjQrL6+vtkymvajN910U3HanDlzisetWbNmFaffe++9LfYtxx57bNanT5/szTffbLbciRMnZt26dSvufy+77LIsIrIZM2YU51m+fHk2cODAjzyWvf3221lEZJ///OfXOE+WZdmYMWOyiMjefffdLMtaHn+arL7/zbIs69SpU3bUUUetcd4xY8Y0m37SSSdlEZE9+eSTWZb9379Rar+7+jFv6tSpzT7fsPFzG9IaTJ8+PTbddNPYe++9I+KDy3ATJkyIW265pdmtLnfddVcMGTIkdtttt+K02tramDx5crPlde3aNQ444ICYMWNGs1s/fvnLX8aQIUOKt1Tcdttt0djYGOPHj48333yz+N9mm20W2267bTz44IPNlltVVRVf+tKXmk1rOoN75513xqpVq9b4Gj989WPJkiXxzjvvxLBhw+KJJ55oMe/w4cNjhx12WOOyUstctWpVvPXWWzFw4MDo3r17crn/jhNPPLHZn4cNGxb//Oc/W8w3duzYZt8X2G233WL33XdPnoVe1956662IaH42LuKD99O9994bF1xwQfTo0SNuvvnmOPnkk6Nfv34xYcKEZt9ZyLtNjz322GaXqHfffffIsiyOPfbY4rTy8vLYdddd19n2yvu+LdV7770XER+871fX9P2ZpnmaNG37N998c52MAT4J9ttvv6itrY0tt9wyxo0bF506dYo77rijePvP4sWL44EHHojx48fH0qVLiz/zb731VowcOTLmzp271qcnNTQ0xO9+97sYO3Zss1t0+/TpE4cffng89NBD8e677zbLHH/88VFeXt5iWZ07d46JEycW/7z99ttH9+7d49Of/nSzqxNN/9+038uyLG699dY4+OCDI8uyZvuukSNHxjvvvFPcx951113Rp0+fGDduXHF5NTU1LR5ckbJ06dKIiI98yELT36/+uteFk08+udmfTz311IiINjkO8vGnLCQ0NDTELbfcEnvvvXfMmzcvXnjhhXjhhRdi9913jwULFsTvf//74rwvvfRSbLvtti2Wsf3227eYNmHChHjllVfikUceiYgP7v2cPXt2TJgwoTjP3LlzI8uy2HbbbaO2trbZf//4xz9i4cKFzZbZt2/fZk9/ifjgg/2hhx4aU6ZMiV69esXnP//5mDZtWot7K++8884YMmRIdOzYMXr27Fm8zeadd95pMfb+/fu3Yst98EHt29/+dmy55ZZRVVUVvXr1itra2nj77beTyy1Vx44dW1z27NGjRyxZsqTFvKl/n+22265N77H8cEFsUlVVFd/85jfjH//4R7z++utx8803x5AhQ4q3ezXJu00/fC9/RES3bt0iImLLLbdsMX1dba+879tSNRWn1H3CTfffrn4LYNO239B/LwRsSK644oq47777YubMmXHggQfGm2++2aykv/DCC5FlWXzrW99q8TPf9CCQtf3cL1q0KFasWJE8Vn7605+OxsbGFt93WtNxaIsttmjx892tW7fkPi8iivu9RYsWxdtvv138HsGH/2s6Cdf0Gl566aUYOHBgi/Wkxr+6phLQVBrWpLWlohSr79cHDBgQZWVlvmtAq/jOQsIDDzwQb7zxRtxyyy1xyy23tPj76dOnx/777597uQcffHDU1NTEjBkzYs8994wZM2ZEWVlZHHbYYcV5Ghsbo1AoxN13373GMygftvoHo4gPPhTNnDkzZs2aFb/97W/j3nvvjWOOOSYuueSSmDVrVnTu3Dn+/Oc/x5gxY2KvvfaKK6+8Mvr06ROVlZUxbdq0uOmmm1osM7WelFNPPTWmTZsWZ5xxRuyxxx7FX8AzceLEFl/6/nekts2/o1AoJD/Qf/gqUimavqi8ZMmS4hm5lD59+sTEiRPj0EMPjR133DFmzJgR1113XVRUVOTepmvaNqnpqddcirzv21L16dMnIiLeeOONFn/3xhtvRM+ePVtcdWj6YNB0by3w0Xbbbbfid4PGjh0bQ4cOjcMPPzyee+656Ny5c3Hfc+aZZ8bIkSOTy1jXjxpf03Eozz4v4v/2e02v4Ytf/GIcddRRyXnX9p201urWrVv06dMnnnrqqbXO99RTT0Xfvn2ja9euEbHmExz/7nEptez1uS4+/pSFhOnTp0fv3r3jiiuuaPF3t912W9x+++3xk5/8JKqrq6Nfv34xd+7cFvM999xzLaZ16tQpRo8eHb/61a/i0ksvjV/+8pcxbNiw2HzzzYvzDBgwILIsi/79+8d22233b72OIUOGxJAhQ+LCCy+Mm266KSZPnhy33HJLHHfccXHrrbdGx44d495772324WratGn/1jpnzpwZRx11VFxyySXFaXV1de36KNDUv8/zzz/f7EvaPXr0SN6S89JLLzX7c96z002/K2HevHmx0047feT8lZWVsfPOO8fcuXOLt/G09TZtzfZa3bp8365N3759o7a2NvlL+h599NH47Gc/22L6vHnzildjgPzKy8vju9/9buy9995x+eWXxznnnFO8daiysrLF095ao7a2NmpqapLHyjlz5kRZWVmLKwPrWm1tbXTp0iUaGho+8jX069cvnnnmmciyrNlxIDX+lNGjR8dPf/rTeOihh2Lo0KEt/v7Pf/5zzJ8/P0444YTitB49eiT386sflyI++tg0d+7cZldmXnjhhWhsbCzu15tu11x9faWsi42P25BW895778Vtt90Wo0ePjnHjxrX475RTTomlS5cWHw134IEHxqxZs+LRRx8tLmPRokVrfMzqhAkT4vXXX4+f/exn8eSTTza7BSnigycNlJeXx5QpU1qc9c2yrHgP/NosWbKkRbbpQ1TT7Rvl5eVRKBRaPGr0331qTHl5eYt1//jHP27XsxO//vWvm907++ijj8Zf//rXOOCAA4rTBgwYEHPmzGn2W4CffPLJ+Mtf/tJsWU1PHmrtB/XBgwdHhw4dWny4nTt3brz88sst5n/77bfjkUceiR49ehQ/3Lb1Nm3N9lrdunjfttahhx4ad955Z7NbFH7/+9/H888/3+wqXZPZs2fHHnvssc7WD59EI0aMiN122y0uu+yyqKuri969e8eIESPi6quvTl7pW/03qq+uvLw89t9///jNb37T7FaYBQsWxE033RRDhw4tnmFfX8rLy+PQQw+NW2+9Nfl46A+/hgMPPDBef/31Zo8Xb3oUamucddZZUV1dHSeccEKL/eHixYvjxBNPjJqamuJjSSM+OC698847za5IvPHGG3H77be3WH6nTp3Welxa/eTnj3/844iI4n69a9eu0atXr/jTn/7UbL4rr7wyua6I1h8H+fhzZWE1d9xxRyxdujTGjBmT/PshQ4ZEbW1tTJ8+PSZMmBBnn3123HDDDTFq1Kg4/fTTi49O7devX/KS44EHHhhdunSJM888s7ij+rABAwbEBRdcEF//+tdj/vz5MXbs2OjSpUvMmzcvbr/99vjyl78cZ5555lpfw/XXXx9XXnllHHLIITFgwIBYunRp/PSnP42uXbvGgQceGBERBx10UFx66aUxatSoOPzww2PhwoVxxRVXxMCBAz/yUunajB49Om644Ybo1q1b7LDDDvHII4/E/fff3+z3BrS1gQMHxtChQ+MrX/lKvP/++3HZZZfFJptsEmeffXZxnmOOOSYuvfTSGDlyZBx77LGxcOHC+MlPfhI77rhjsy+bVVdXxw477BC//OUvY7vttouePXvGoEGDYtCgQcl1d+zYMfbff/+4//774/zzzy9Of/LJJ+Pwww+PAw44IIYNGxY9e/aM1157La6//vp4/fXX47LLLiteQm/rbdqa7bW6dfG+bfpldK+//npEfPCowVdffTUiPri9rel+42984xvxq1/9Kvbee+84/fTTY9myZTF16tTYaaedWnzZf+HChfHUU0+1+HIfkN9ZZ50Vhx12WFx33XVx4oknxhVXXBFDhw6NnXbaKY4//vjYZpttYsGCBfHII4/Eq6++Gk8++eRal3fBBRfEfffdF0OHDo2TTjopKioq4uqrr47333+/2e8bWJ++973vxYMPPhi77757HH/88bHDDjvE4sWL44knnoj7778/Fi9eHBEffLn68ssvjyOPPDJmz54dffr0iRtuuKHVjwPfdttt4/rrr4/JkyfHTjvt1OI3OL/55ptx8803N3vk6cSJE+NrX/taHHLIIXHaaafFihUr4qqrrortttuuxcMtBg8eHPfff39ceumlsfnmm0f//v2bfbl73rx5MWbMmBg1alQ88sgjceONN8bhhx8en/nMZ4rzHHfccfG9730vjjvuuNh1113jT3/6Uzz//PMtXsvgwYMjIuKb3/xmTJw4MSorK+Pggw/+WP2SPHJqy0cvfRwcfPDBWceOHbPly5evcZ6jjz46q6ysLD5q7amnnsqGDx+edezYMevbt2/2ne98J7v22mvX+GixyZMnZxGR7bfffmtcx6233poNHTo069SpU9apU6fsU5/6VHbyySdnzz33XHGe4cOHZzvuuGOL7BNPPJFNmjQp22qrrbKqqqqsd+/e2ejRo7PHH3+82XzXXntttu2222ZVVVXZpz71qWzatGnJR7JFRHbyyScnxxmrPVJtyZIl2Ze+9KWsV69eWefOnbORI0dmc+bMyfr169fssW7/7qNTO3Xq1GLe1cf+4cf1XXLJJdmWW26ZVVVVZcOGDSs+Lu7DbrzxxmybbbbJOnTokH32s5/N7r333uSj6x5++OFs8ODBWYcOHVr1GNXbbrstKxQK2csvv1yctmDBgux73/teNnz48KxPnz5ZRUVF1qNHj2yfffbJZs6c2Szf2m2a2lYf3i6LFi1qNn317Zhne6XeJ1nWuvftmvTr1y+LiOR/q/8cPfPMM9n++++f1dTUZN27d88mT56c/etf/2qxzKuuuiqrqakpPooQWLs17UeyLMsaGhqyAQMGZAMGDCg+wvTFF1/MjjzyyGyzzTbLKisrs759+2ajR49uth9b06NTs+yD49XIkSOzzp07ZzU1Ndnee++dPfzww60e05qOg/369csOOuigFtNTx7MFCxZkJ598crbllltmlZWV2WabbZbtu+++2TXXXNNsvpdeeikbM2ZMVlNTk/Xq1Ss7/fTTs3vuuadVx7ImTz31VDZp0qSsT58+xXVNmjSpxeO1m/zud7/LBg0alHXo0CHbfvvtsxtvvDG5/50zZ0621157ZdXV1VlEFI8NTfP+/e9/z8aNG5d16dIl69GjR3bKKadk7733XrNlrFixIjv22GOzbt26ZV26dMnGjx+fLVy4MHmc+853vpP17ds3Kysr8xjVT4BClq2jbzjCBmb+/PnRv3//mDp16kee1V6fGhoaYocddojx48fHd77znXYbx0fZULbXurTLLrvEiBEj4oc//GF7DwWgzZ133nkxZcqUWLRokYc8UDLfWYD1rLy8PM4///y44oorYtmyZe09nE+Me+65J+bOnRtf//rX23soAPCxpSxAG5gwYUIsXrx4nT1ClI82atSoWLZsWfTu3bu9hwIAH1vKAgAAkOQ7CwAAQJIrCwAAQJKyAAAAJCkLAABAUqt/g3OhUFif4wBgHfJ1tHWjlO3oeAlsTFxZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIKmivQcAAG2hQ4cObZLp169f7kyhUMidybIsd6YUjY2NuTOljK2UbVBeXp4709DQkDtTV1eXO/P666/nzpRq0KBBuTN9+/bNnTnnnHNyZ7p37547M3ny5NyZBQsW5M506dIld6aUfUJtbW3uzIknnpg7M378+NyZ1nBlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEhSFgAAgKSK9h4AALSFxsbG3JmVK1fmzixevDh3phRZlm2w6+nQoUPuTCnbeunSpbkzXbt2zZ2pq6vLnSnl9ZRq+fLluTP9+vXLnZk6dWruzNe+9rXcmX/+85+5M6X8fDc0NOTOlJXlP89eX1+fO/P444/nzowfPz53pjVcWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACCpor0HAAAbqkKh0CaZLMvaZD2lKGU9jY2NuTOrVq3KnenWrVvuzBNPPJE7U1tbmzuzZMmS3JmIiK5du+bOVFVVlbSutvDKK6/kztTV1eXOlPIzVFlZmTtTXV2dO1PKe3vFihW5M+uLKwsAAECSsgAAACQpCwAAQJKyAAAAJCkLAABAkrIAAAAkKQsAAECSsgAAACQpCwAAQJKyAAAAJCkLAABAkrIAAAAkVbT3AIA1y7Isd6ZQKKyHkQAbg1L2KaUoK8t/LrKmpiZ3ZtCgQbkzdXV1uTNHHHFE7szPf/7z3JmI0rZdW1m1alXuTMeOHXNnStkGDQ0NbbKetjrGlpeXt8l6WmPDfUcCAADtSlkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkiraewDwSZFlWXsPAT7RCoXCRpUpZZ9SynraSmNjY+7M448/njuzyy675M70798/d6a6ujp3ZkN3/PHH58689957uTP19fW5M5WVlbkzFRX5PwaXleU/z17Kz115eXnuzPriygIAAJCkLAAAAEnKAgAAkKQsAAAAScoCAACQpCwAAABJygIAAJCkLAAAAEnKAgAAkKQsAAAAScoCAACQpCwAAABJFe09AABoC1mWtUmmFG21nsbGxjZZT1lZ/nORhUIhd6ampiZ3ZsGCBbkzTz/9dO7Mxui5557Lnamrq8udqajI//G0lPdPfX197kwpY/u4c2UBAABIUhYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACApIr2HgB83GRZ1t5DAChpX1QoFHJnKiryf1QoK8t/LvLtt9/OnenSpUvuzD777JM706NHj9yZjdGjjz7aJuuprKzMnenevXvuTGNjY5tkPu6fG1xZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIKmivQcArNmBBx7Y3kMANiLl5eW5M1mW5c4sXbo0d6ZDhw65M2PHjs2d+ctf/pI7U1dXlzvDBwqFQu5MRUX+j6eNjY25M/X19bkzpbxPS9kGGxJXFgAAgCRlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEiqaO8BAGt29913t/cQgI1IfX197syqVatyZzp06JA7c/jhh+fONDY25s787//+b+4Mpevbt2+brKeU93aWZRtsZkPiygIAAJCkLAAAAEnKAgAAkKQsAAAAScoCAACQpCwAAABJygIAAJCkLAAAAEnKAgAAkKQsAAAAScoCAACQpCwAAABJFe09AAAgvyzLcmeqqqpyZ8rK8p9XbGxszJ3585//nDsze/bs3BlKV1lZmTuzatWqNllPKe/TtlIoFNp7CP+WDXfLAgAA7UpZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJIq2nsA0J4efPDBNlvXcccd12brAjZ+ZWX5z/dlWZY7s2rVqtyZUvTp0yd35q233sqd6dGjR+4MH1i5cmXuzMKFC3NnunTpkjvTvXv33JlCodAmmY87VxYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIqmjvAUB7GjFiRJut69prr22zdQEfL1mWbbDrqaqqyp3Zeeedc2cWL16cO/OZz3wmd+bll1/OndnQlfLvWigUcmfKy8tzZ0oZWynraWxszJ3ZkH/uNiSuLAAAAEnKAgAAkKQsAAAAScoCAACQpCwAAABJygIAAJCkLAAAAEnKAgAAkKQsAAAAScoCAACQpCwAAABJygIAAJBU0d4DgHUly7L2HgJANDY25s6Usv8qLy/PnSnFsmXLcmdmz56dO1NXV5c7s3LlytyZjVFVVVXuTN++fXNn6uvrc2c6d+6cO9OlS5fcmYaGhtwZWseVBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJIq2nsAALAxKRQKuTPl5eW5Mw0NDbkz7777bu5MZWVl7kxNTU3uzJIlS3JnKip8jImI6N69e+7M8uXLc2dKeZ82Njbmzrz33nu5M6W8T2kdVxYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIUhYAAICkivYeAHzcrFy5sr2HAGzACoVC7kyWZbkz9fX1uTPdunXLnbnyyitzZ7bffvvcmUGDBuXO1NXV5c5sjBYvXpw7U8p7rrKyMndmk002yZ0pK8t/LnvVqlW5M7SOKwsAAECSsgAAACQpCwAAQJKyAAAAJCkLAABAkrIAAAAkKQsAAECSsgAAACQpCwAAQJKyAAAAJCkLAABAkrIAAAAkVbT3AODj5t57723vIQAbsCzLcmfKyvKfu+vQoUPuTPfu3XNnXnzxxdyZ0047LXemFF26dGmT9WzoCoVC7kwp79OOHTu2yXpWrVqVO8P648oCAACQpCwAAABJygIAAJCkLAAAAEnKAgAAkKQsAAAAScoCAACQpCwAAABJygIAAJCkLAAAAEnKAgAAkKQsAAAASRXtPQBIybKsvYewRmPGjGnvIQCUZNGiRbkzl156ae7M5z//+dyZyy+/PHeGD9TX1+fOVFVV5c506dIld6asLP956YaGhtyZQqGQO0PruLIAAAAkKQsAAECSsgAAACQpCwAAQJKyAAAAJCkLAABAkrIAAAAkKQsAAECSsgAAACQpCwAAQJKyAAAAJCkLAABAUkV7DwBSCoVC7kyWZW2yHoB1rZT9V1lZ/vN9pezz3nzzzdyZK6+8Mnfm8ssvz53ZGLXVsayxsTF3pr6+PnemosJHzY87VxYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIqmjvAUBKlmVtsp6vfe1ruTPf//7318NIgI1FoVDInWloaMidWb58ee5Mnz59cmdmzpyZO/PGG2/kzmxsli1b1t5DWKuKivwfAUvJlHI8LyVTys8drePKAgAAkKQsAAAAScoCAACQpCwAAABJygIAAJCkLAAAAEnKAgAAkKQsAAAAScoCAACQpCwAAABJygIAAJCkLAAAAEkV7T0AaE/f//7323sIwEYmy7LcmY4dO+bO1NfX587U1dXlztx33325M+eff37uTCnbbUO2fPnyknLvv/9+7kyHDh1yZwqFQu5MQ0NDm6ynlAzrjysLAABAkrIAAAAkKQsAAECSsgAAACQpCwAAQJKyAAAAJCkLAABAkrIAAAAkKQsAAECSsgAAACQpCwAAQJKyAAAAJFW09wAAYGNSVpb/PFyhUGiTzIoVK3Jn5s+fnztTytg2NgMGDCgpt2rVqtyZ+vr63JnOnTvnzlRWVubOlPJ6sizLndmQ33Mb8thaw5UFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkiraewBs/LIsa+8hAERZWf7zY+Xl5bkz1dXVuTP19fW5M5WVlbkzffv2zZ057LDDcmemTJmSO0PpqqqqcmdKef9UVOT/2FjKejZkpfx819TU5M506tQpd2Z9cWUBAABIUhYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACApEKWZVmrZiwU1vdYoKiVb8tmvEfh/5TyM/RxUsrrKyvLf36slP1KW62nFA0NDbkzbfVe2tjes6X+m5aXl6/jkaS11Xuurf5dS3k9bfVZo5R9Qu/evXNnXnnlldyZ1nBlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEhSFgAAgCRlAQAASFIWAACAJGUBAABIUhYAAIAkZQEAAEhSFgAAgKSK9h4ApBQKhfYeAlCCxsbG3Jmysvznrerr63NnSpFlWe5MKduglPXQdkr5Ny1VQ0NDm6ynrY6zpby3SxlbW/0MlTK2j/s+wZUFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkgpZlmWtmrFQWN9jAWAdaeWuHQDWypUFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAgSVkAAACSKlo7Y5Zl63McAADABsaVBQAAIElZAAAAkpQFAAAgSVkAAACSlAUAACBJWQAAAJKUBQAAIElZAAAAkpQFAAAg6f8DWzDyrMfFOAgAAAAASUVORK5CYII=\n"},"metadata":{}}],"execution_count":100},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_mnist(images):\n    mean = torch.tensor([0.1307], device=images.device)\n    std = torch.tensor([0.3081], device=images.device)\n    return (images - mean.view(1, -1, 1, 1)) / std.view(1, -1, 1, 1)\n\ndevice = next(classifier.model.parameters()).device\n\n# Load and convert adversarial data\nadv_imgs = load_obj(\"fgsm_attack\", directory='/kaggle/working/attack_data/')\nadv_labels = load_obj(\"fgsm_labels\", directory='/kaggle/working/attack_data/')\n\n# Convert to tensors and move to device\nadv_imgs = adv_imgs.to(device) if isinstance(adv_imgs, torch.Tensor) else torch.tensor(adv_imgs).to(device)\nadv_labels = adv_labels.to(device) if isinstance(adv_labels, torch.Tensor) else torch.tensor(adv_labels).to(device)\nadv_imgs = normalize_mnist(adv_imgs)\n\n# Apply reformer (ensure it outputs to device)\nreformed_adv = reformer.heal(adv_imgs).to(device)  # Remove .cpu() in reformer.heal()\n\n# Get predictions\nwith torch.no_grad():\n    preds_before = classifier.model(adv_imgs).argmax(dim=1)\n    preds_after = classifier.model(reformed_adv).argmax(dim=1)\n\n# Calculate accuracy\nadv_accuracy_before = (preds_before == adv_labels).float().mean().item()\nadv_accuracy_after = (preds_after == adv_labels).float().mean().item()\n\n_, test_labels = next(iter(dataset.test_loader))  # Get batch of normal data\ntest_imgs, test_labels = next(iter(dataset.test_loader))\ntest_imgs, test_labels = test_imgs.to(device), test_labels.to(device)\ntest_imgs = normalize_mnist(test_imgs)\n# Apply reformer\nreformed_normal = reformer.heal(test_imgs)\n\n# Get predictions\nwith torch.no_grad():\n    normal_preds_before = classifier.model(test_imgs).argmax(dim=1)\n    normal_preds_after = classifier.model(reformed_normal).argmax(dim=1)\n\n# Calculate normal accuracy\nnormal_accuracy_before = (normal_preds_before == test_labels).float().mean().item()\nnormal_accuracy_after = (normal_preds_after == test_labels).float().mean().item()\n\n# ===== Print Results =====\nprint(\"Adversarial Images:\")\nprint(f\"Accuracy before reforming: {adv_accuracy_before:.2%}\")\nprint(f\"Accuracy after reforming: {adv_accuracy_after:.2%}\\n\")\n\nprint(\"Normal Images:\")\nprint(f\"Accuracy before reforming: {normal_accuracy_before:.2%}\")\nprint(f\"Accuracy after reforming: {normal_accuracy_after:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T17:37:29.529850Z","iopub.execute_input":"2025-06-07T17:37:29.530086Z","iopub.status.idle":"2025-06-07T17:37:30.442142Z","shell.execute_reply.started":"2025-06-07T17:37:29.530070Z","shell.execute_reply":"2025-06-07T17:37:30.441458Z"}},"outputs":[{"name":"stdout","text":"Adversarial Images:\nAccuracy before reforming: 93.20%\nAccuracy after reforming: 69.00%\n\nNormal Images:\nAccuracy before reforming: 100.00%\nAccuracy after reforming: 78.12%\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}